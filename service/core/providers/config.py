config = {
    "openai": [
        {
            "model_name": "gpt-5-nano",
            "litellm_provider": "openai",
            "cache_read_input_token_cost": 5e-09,
            "cache_read_input_token_cost_flex": 2.5e-09,
            "input_cost_per_token": 5e-08,
            "input_cost_per_token_flex": 2.5e-08,
            "input_cost_per_token_priority": 2.0 / 5e-06,
            "max_input_tokens": 272000,
            "max_output_tokens": 128000,
            "max_tokens": 128000,
            "mode": "chat",
            "output_cost_per_token": 4e-07,
            "output_cost_per_token_flex": 2e-07,
            "supported_endpoints": ["/v1/chat/completions", "/v1/batch", "/v1/responses"],
            "supported_modalities": ["text", "image"],
            "supported_output_modalities": ["text"],
            "supports_function_calling": True,
            "supports_native_streaming": True,
            "supports_parallel_function_calling": True,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_reasoning": True,
            "supports_response_schema": True,
            "supports_system_messages": True,
            "supports_tool_choice": True,
            "supports_vision": True,
        },
        {
            "model_name": "gpt-4o-mini",
            "litellm_provider": "openai",
            "cache_read_input_token_cost": 7.5e-08,
            "cache_read_input_token_cost_priority": 1.25e-07,
            "input_cost_per_token": 1.5e-07,
            "input_cost_per_token_batches": 7.5e-08,
            "input_cost_per_token_priority": 2.5e-07,
            "max_input_tokens": 128000,
            "max_output_tokens": 16384,
            "max_tokens": 16384,
            "mode": "chat",
            "output_cost_per_token": 6e-07,
            "output_cost_per_token_batches": 3e-07,
            "output_cost_per_token_priority": 1e-06,
            "supports_function_calling": True,
            "supports_parallel_function_calling": True,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_response_schema": True,
            "supports_system_messages": True,
            "supports_tool_choice": True,
            "supports_service_tier": True,
            "supports_vision": True,
        },
        {
            "model_name": "o1-mini",
            "litellm_provider": "openai",
            "cache_read_input_token_cost": 5.5e-07,
            "input_cost_per_token": 1.1e-06,
            "max_input_tokens": 128000,
            "max_output_tokens": 65536,
            "max_tokens": 65536,
            "mode": "chat",
            "output_cost_per_token": 4.4e-06,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_vision": True,
        },
        {
            "model_name": "gpt-5",
            "litellm_provider": "openai",
            "cache_read_input_token_cost": 1.25e-07,
            "cache_read_input_token_cost_flex": 6.25e-08,
            "cache_read_input_token_cost_priority": 2.5e-07,
            "input_cost_per_token": 1.25e-06,
            "input_cost_per_token_flex": 6.25e-07,
            "input_cost_per_token_priority": 2.5e-06,
            "max_input_tokens": 272000,
            "max_output_tokens": 128000,
            "max_tokens": 128000,
            "mode": "chat",
            "output_cost_per_token": 1e-05,
            "output_cost_per_token_flex": 5e-06,
            "output_cost_per_token_priority": 2e-05,
            "supported_endpoints": ["/v1/chat/completions", "/v1/batch", "/v1/responses"],
            "supported_modalities": ["text", "image"],
            "supported_output_modalities": ["text"],
            "supports_function_calling": True,
            "supports_native_streaming": True,
            "supports_parallel_function_calling": True,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_reasoning": True,
            "supports_response_schema": True,
            "supports_system_messages": True,
            "supports_tool_choice": True,
            "supports_service_tier": True,
            "supports_vision": True,
        },
        {
            "model_name": "gpt-4o",
            "litellm_provider": "openai",
            "cache_read_input_token_cost": 1.25e-06,
            "cache_read_input_token_cost_priority": 2.125e-06,
            "input_cost_per_token": 2.5e-06,
            "input_cost_per_token_batches": 1.25e-06,
            "input_cost_per_token_priority": 4.25e-06,
            "max_input_tokens": 128000,
            "max_output_tokens": 16384,
            "max_tokens": 16384,
            "mode": "chat",
            "output_cost_per_token": 1e-05,
            "output_cost_per_token_batches": 5e-06,
            "output_cost_per_token_priority": 1.7e-05,
            "supports_function_calling": True,
            "supports_parallel_function_calling": True,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_response_schema": True,
            "supports_system_messages": True,
            "supports_tool_choice": True,
            "supports_service_tier": True,
            "supports_vision": True,
        },
        {
            "model_name": "o1-preview",
            "litellm_provider": "openai",
            "cache_read_input_token_cost": 7.5e-06,
            "input_cost_per_token": 1.5e-05,
            "max_input_tokens": 128000,
            "max_output_tokens": 32768,
            "max_tokens": 32768,
            "mode": "chat",
            "output_cost_per_token": 6e-05,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_reasoning": True,
            "supports_vision": True,
        },
    ],
    "openai-azure": [
        {
            "model_name": "azure/gpt-4o-mini",
            "litellm_provider": "azure",
            "cache_read_input_token_cost": 7.5e-08,
            "input_cost_per_token": 1.65e-07,
            "max_input_tokens": 128000,
            "max_output_tokens": 16384,
            "max_tokens": 16384,
            "mode": "chat",
            "output_cost_per_token": 6.6e-07,
            "supports_function_calling": True,
            "supports_parallel_function_calling": True,
            "supports_prompt_caching": True,
            "supports_response_schema": True,
            "supports_tool_choice": True,
            "supports_vision": True,
        },
        {
            "model_name": "azure/gpt-5",
            "litellm_provider": "azure",
            "cache_read_input_token_cost": 1.25e-07,
            "input_cost_per_token": 1.25e-06,
            "max_input_tokens": 272000,
            "max_output_tokens": 128000,
            "max_tokens": 128000,
            "mode": "chat",
            "output_cost_per_token": 1e-05,
            "supported_endpoints": ["/v1/chat/completions", "/v1/batch", "/v1/responses"],
            "supported_modalities": ["text", "image"],
            "supported_output_modalities": ["text"],
            "supports_function_calling": True,
            "supports_native_streaming": True,
            "supports_parallel_function_calling": True,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_reasoning": True,
            "supports_response_schema": True,
            "supports_system_messages": True,
            "supports_tool_choice": True,
            "supports_vision": True,
        },
        {
            "model_name": "azure/gpt-4o",
            "litellm_provider": "azure",
            "cache_read_input_token_cost": 1.25e-06,
            "input_cost_per_token": 2.5e-06,
            "max_input_tokens": 128000,
            "max_output_tokens": 16384,
            "max_tokens": 16384,
            "mode": "chat",
            "output_cost_per_token": 1e-05,
            "supports_function_calling": True,
            "supports_parallel_function_calling": True,
            "supports_prompt_caching": True,
            "supports_response_schema": True,
            "supports_tool_choice": True,
            "supports_vision": True,
        },
    ],
    "google": [
        {
            "model_name": "gemini/gemini-2.5-flash",
            "litellm_provider": "gemini",
            "cache_read_input_token_cost": 3e-08,
            "input_cost_per_audio_token": 1e-06,
            "input_cost_per_token": 3e-07,
            "max_audio_length_hours": 8.4,
            "max_audio_per_prompt": 1,
            "max_images_per_prompt": 3000,
            "max_input_tokens": 1048576,
            "max_output_tokens": 65535,
            "max_pdf_size_mb": 30,
            "max_tokens": 65535,
            "max_video_length": 1,
            "max_videos_per_prompt": 10,
            "mode": "chat",
            "output_cost_per_reasoning_token": 2.5e-06,
            "output_cost_per_token": 2.5e-06,
            "rpm": 100000,
            "source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview",
            "supported_endpoints": ["/v1/chat/completions", "/v1/completions", "/v1/batch"],
            "supported_modalities": ["text", "image", "audio", "video"],
            "supported_output_modalities": ["text"],
            "supports_audio_output": False,
            "supports_function_calling": True,
            "supports_parallel_function_calling": True,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_reasoning": True,
            "supports_response_schema": True,
            "supports_system_messages": True,
            "supports_tool_choice": True,
            "supports_url_context": True,
            "supports_vision": True,
            "supports_web_search": True,
            "tpm": 8000000,
        },
        {
            "model_name": "gemini/gemini-2.5-pro",
            "litellm_provider": "gemini",
            "cache_read_input_token_cost": 3.125e-07,
            "input_cost_per_token": 1.25e-06,
            "input_cost_per_token_above_200k_tokens": 2.5e-06,
            "max_audio_length_hours": 8.4,
            "max_audio_per_prompt": 1,
            "max_images_per_prompt": 3000,
            "max_input_tokens": 1048576,
            "max_output_tokens": 65535,
            "max_pdf_size_mb": 30,
            "max_tokens": 65535,
            "max_video_length": 1,
            "max_videos_per_prompt": 10,
            "mode": "chat",
            "output_cost_per_token": 1e-05,
            "output_cost_per_token_above_200k_tokens": 1.5e-05,
            "rpm": 2000,
            "source": "https://cloud.google.com/vertex-ai/generative-ai/pricing",
            "supported_endpoints": ["/v1/chat/completions", "/v1/completions"],
            "supported_modalities": ["text", "image", "audio", "video"],
            "supported_output_modalities": ["text"],
            "supports_audio_input": True,
            "supports_function_calling": True,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_reasoning": True,
            "supports_response_schema": True,
            "supports_system_messages": True,
            "supports_tool_choice": True,
            "supports_video_input": True,
            "supports_vision": True,
            "supports_web_search": True,
            "tpm": 800000,
        },
        {
            "model_name": "gemini/gemini-3-pro-preview",
            "litellm_provider": "gemini",
            "cache_read_input_token_cost": 2e-07,
            "cache_read_input_token_cost_above_200k_tokens": 4e-07,
            "input_cost_per_token": 2e-06,
            "input_cost_per_token_above_200k_tokens": 4e-06,
            "input_cost_per_token_batches": 1e-06,
            "max_audio_length_hours": 8.4,
            "max_audio_per_prompt": 1,
            "max_images_per_prompt": 3000,
            "max_input_tokens": 1048576,
            "max_output_tokens": 65535,
            "max_pdf_size_mb": 30,
            "max_tokens": 65535,
            "max_video_length": 1,
            "max_videos_per_prompt": 10,
            "mode": "chat",
            "output_cost_per_token": 1.2e-05,
            "output_cost_per_token_above_200k_tokens": 1.8e-05,
            "output_cost_per_token_batches": 6e-06,
            "rpm": 2000,
            "source": "https://cloud.google.com/vertex-ai/generative-ai/pricing",
            "supported_endpoints": ["/v1/chat/completions", "/v1/completions", "/v1/batch"],
            "supported_modalities": ["text", "image", "audio", "video"],
            "supported_output_modalities": ["text"],
            "supports_audio_input": True,
            "supports_function_calling": True,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_reasoning": True,
            "supports_response_schema": True,
            "supports_system_messages": True,
            "supports_tool_choice": True,
            "supports_video_input": True,
            "supports_vision": True,
            "supports_web_search": True,
            "tpm": 800000,
        },
    ],
    "google-vertex": [
        {
            "model_name": "gemini-2.5-flash",
            "litellm_provider": "vertex_ai-language-models",
            "cache_read_input_token_cost": 3e-08,
            "input_cost_per_audio_token": 1e-06,
            "input_cost_per_token": 3e-07,
            "max_audio_length_hours": 8.4,
            "max_audio_per_prompt": 1,
            "max_images_per_prompt": 3000,
            "max_input_tokens": 1048576,
            "max_output_tokens": 65535,
            "max_pdf_size_mb": 30,
            "max_tokens": 65535,
            "max_video_length": 1,
            "max_videos_per_prompt": 10,
            "mode": "chat",
            "output_cost_per_reasoning_token": 2.5e-06,
            "output_cost_per_token": 2.5e-06,
            "source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview",
            "supported_endpoints": ["/v1/chat/completions", "/v1/completions", "/v1/batch"],
            "supported_modalities": ["text", "image", "audio", "video"],
            "supported_output_modalities": ["text"],
            "supports_audio_output": False,
            "supports_function_calling": True,
            "supports_parallel_function_calling": True,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_reasoning": True,
            "supports_response_schema": True,
            "supports_system_messages": True,
            "supports_tool_choice": True,
            "supports_url_context": True,
            "supports_vision": True,
            "supports_web_search": True,
        },
        {
            "model_name": "gemini-2.5-pro",
            "litellm_provider": "vertex_ai-language-models",
            "cache_read_input_token_cost": 1.25e-07,
            "cache_creation_input_token_cost_above_200k_tokens": 2.5e-07,
            "input_cost_per_token": 1.25e-06,
            "input_cost_per_token_above_200k_tokens": 2.5e-06,
            "max_audio_length_hours": 8.4,
            "max_audio_per_prompt": 1,
            "max_images_per_prompt": 3000,
            "max_input_tokens": 1048576,
            "max_output_tokens": 65535,
            "max_pdf_size_mb": 30,
            "max_tokens": 65535,
            "max_video_length": 1,
            "max_videos_per_prompt": 10,
            "mode": "chat",
            "output_cost_per_token": 1e-05,
            "output_cost_per_token_above_200k_tokens": 1.5e-05,
            "source": "https://cloud.google.com/vertex-ai/generative-ai/pricing",
            "supported_endpoints": ["/v1/chat/completions", "/v1/completions"],
            "supported_modalities": ["text", "image", "audio", "video"],
            "supported_output_modalities": ["text"],
            "supports_audio_input": True,
            "supports_function_calling": True,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_reasoning": True,
            "supports_response_schema": True,
            "supports_system_messages": True,
            "supports_tool_choice": True,
            "supports_video_input": True,
            "supports_vision": True,
            "supports_web_search": True,
        },
        {
            "model_name": "gemini-3-pro-preview",
            "litellm_provider": "vertex_ai-language-models",
            "cache_read_input_token_cost": 2e-07,
            "cache_read_input_token_cost_above_200k_tokens": 4e-07,
            "cache_creation_input_token_cost_above_200k_tokens": 2.5e-07,
            "input_cost_per_token": 2e-06,
            "input_cost_per_token_above_200k_tokens": 4e-06,
            "input_cost_per_token_batches": 1e-06,
            "max_audio_length_hours": 8.4,
            "max_audio_per_prompt": 1,
            "max_images_per_prompt": 3000,
            "max_input_tokens": 1048576,
            "max_output_tokens": 65535,
            "max_pdf_size_mb": 30,
            "max_tokens": 65535,
            "max_video_length": 1,
            "max_videos_per_prompt": 10,
            "mode": "chat",
            "output_cost_per_token": 1.2e-05,
            "output_cost_per_token_above_200k_tokens": 1.8e-05,
            "output_cost_per_token_batches": 6e-06,
            "source": "https://cloud.google.com/vertex-ai/generative-ai/pricing",
            "supported_endpoints": ["/v1/chat/completions", "/v1/completions", "/v1/batch"],
            "supported_modalities": ["text", "image", "audio", "video"],
            "supported_output_modalities": ["text"],
            "supports_audio_input": True,
            "supports_function_calling": True,
            "supports_pdf_input": True,
            "supports_prompt_caching": True,
            "supports_reasoning": True,
            "supports_response_schema": True,
            "supports_system_messages": True,
            "supports_tool_choice": True,
            "supports_video_input": True,
            "supports_vision": True,
            "supports_web_search": True,
        },
    ],
}
